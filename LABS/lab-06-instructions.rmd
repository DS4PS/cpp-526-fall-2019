---
title: 'Lab 10 - Dataset Joins'
output:
  html_document:
    theme: readable
    df_print: paged
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=F, warning=F)
library( dplyr )
library( pander )
```

#### [DS4PS](https://ds4ps.github.io/course_website/)

<br>

This lab demonstrates the process of joining two datasets through a **merge()** function. This is one of the most common and most important processes in data science, because it allows you to combine multiple datasets to link observations that pertain to the same individuals / cases. The deepest insights often come from bringing data together in this way.

We will use the following packages for the lab:

```{r, eval=F}
library( dplyr )
```

You can create a new RMarkdown file, or download the lab template: 

[RMD Template](https://cdn.rawgit.com/DS4PS/Data-Science-Class/c464d527/TEMPLATES/MarkdownTemplateSimple.Rmd)

<br><br>

# Overview of Joins

<br>

## Inner, Outer, Left and Right

There are two things to remember when conducting joins. The first is that when you merge two datasets it is rare for them both to contain the exact same observations. As a result, you need to make decisions about which data to keep and which data to drop post-merge. There are four options:

* Keep everything: "Outer Join"
* Keep only observations in both datasets: "Inner Join"
* Keep all observations from dat1: "Left Join"
* Keep all observations from dat2: "Right Join"

<br>

```{r, fig.cap="", echo=F, out.width='40%', fig.align='center' }
knitr::include_graphics( "figures/shared_observations.png" )
```

<br>

These options are specified through the *all=*, *all.x=*, and *all.y=* arguments in the **merge()** function, where the arguments all.x stands for observations in dat1, and all.y stands for observations in dat2 in this example.


```{r, fig.cap="", echo=F, out.width='80%', fig.align='center' }
knitr::include_graphics( "figures/outer_join.png" )
```

<br>

```{r, fig.cap="", echo=F, out.width='80%', fig.align='center'}
knitr::include_graphics( "figures/inner_join.png" )
```

<br>

```{r, fig.cap="", echo=F, out.width='80%', fig.align='center' }
knitr::include_graphics( "figures/left_join.png" )
```

<br>

```{r, fig.cap="", echo=F, out.width='80%', fig.align='center' }
knitr::include_graphics( "figures/right_join.png" )
```

 
<br><br>

## Compound IDs

It is often the case where a single ID does not unique specify observations needed to join data. For example, if you are looking at the relationship between employee eye color and their height, these are both variables that can only have one value per employee, so the employee ID would be sufficient to merge the eye color and height datasets.

If we want to look at the relationship between employee sick days in a given year and their performance, we now might have multiple years of data for each employee. To merge these two datasets we need to use both ID and YEAR. This is an example of a compound ID - two or more variables are needed to create a unique key for the join.

```{r, fig.cap="", echo=F, out.width='60%', fig.align='center' }
knitr::include_graphics( "figures/compound_ids.png" )
```


Consider an example where sales representatives get a bonus if they sell over $100,000 in subscriptions each year. We have one database generated by the sales department, and another generated by HR. We want to merge them to ensure bonuses have been properly issued. The data tables are structured as follows:

```{r, echo=F}
id <- c("A","A","B","B","C","C")
year <- c(2001,2002,2001,2002,2001,2002)
sales <- c(54000,119000,141000,102000,66000,68000)
bonus <- c(F,T,T,T,F,F)

dat1 <- data.frame( id, year, sales )
dat2 <- data.frame( id, year, bonus )
dat1 %>% pander()
dat2 %>% pander()
```


The RIGHT way to merge these tables is to specify the set of IDs that allow you to indentify unique observations. The employee ID is not sufficient in this case because there are separate observations for each year. As a result, 

```{r}
merge( dat1, dat2, by=c("id","year") )         %>% pander()
```


The WRONG way to merge these two datasets is to use only the employee ID. In this case, since the rows are now no longer unique, the only choice the merge() function has is to join EVERY instance on the right to each instance on the left. It has the effect of blowing up the size of the database (notice we have increased from 6 to 12 rows), as well as duplicating fields and incorrectly aligning data.

Row number 2, for example, reports that employee A received a bonus on $54,000 in sales.

```{r}
merge( dat1, dat2, by="id" )         %>% pander()
```

The solution is to ensure you are using the combination of keys that ensures each observation is correctly specified. 

<br><br>

## Merge Keys

Your "keys" are the shared IDs across your datasets used for the join. You can check for shared variable names by looking at the intersection of names in both:

```{r}
intersect( names(dat1), names(dat2) )
```

Thi works when the datasets were generated from a common system that uses standardized and identical ID names. In other cases, the same key may be spelled differently ('year' vs. 'YEAR') or have completely different names. 

The check above at least allows you to catch instances where variable names would be repeated, and thus duplicated in the merged file. When this happens, like the 'year' variable in the example above, the merge operation will add an 'x' and 'y' to the end of the variable names to specify which dataset they originated from ('year.x' and 'year.y' in the example above). 

In instances where variable names differ, you can specify the names directly using "by.x=" and "by.y=":

```{r, eval=F}
merge( dat1, dat2, by.x="fips", by.y="FIPS" ) 
```


<br><br>

## In Summary

You will be using the merge() function in this lab to join datasets. You need to specify two arguments in each merge call.

```{r, eval=F}

merge( dat1, dat2, by="", all="" )
```


Your IDs used to join dataset:

* Use "by=" when variable names are identical
* Use "by.x=" and "by.y=" when the variable names are spelled differently
* Remember the c() when you have a compound key:  by=c("FIPS","YEAR")

Specify an outer, inner, left, or right join:

* all=TRUE creates an outer join
* all=FALSE creates an inner join
* all.x=TRUE creates a left join
* all.y=TRUE creates a right join




<br><br>

# Data

This lab uses nonprofit tax data and population data from the US Census. 

Nonprofit data was obtained through the National Center for Charitable Statistics, the 2000 and 2010 Core files: [DATA DICTIONARY](https://nccs-data.urban.org/dd2.php?close=1&form=Core+2010+PC).

Census data is from the 2000 and 2010 Dicennial Census, table SF1, population by county.


```{r, cache=T}
url <- "https://github.com/DS4PS/Data-Science-Class/blob/master/DATA/nonprofits_2010.csv?raw=true"
np.dat.2010 <- read.csv( url, stringsAsFactors=F )
head( np.dat.2010 )
```



```{r, cache=T}
url <- "https://github.com/DS4PS/Data-Science-Class/blob/master/DATA/nonprofits_2000.csv?raw=true"
np.dat.2000 <- read.csv( url, stringsAsFactors=F )
head( np.dat.2000 )
```

```{r}
url <- "https://raw.githubusercontent.com/DS4PS/Data-Science-Class/master/DATA/census_population_2000_and_2010.csv"
pop <- read.csv( url, stringsAsFactors=F )
head( pop )
```


<br><br>

# Lab Instructions

In this lab you will look at nonprofit density by county. Your analysis will mirror the analysis of "All Subsectors" in this table (samples differ slightly so see below for exact solutions):

```{r, fig.cap="", echo=F, out.width='80%', fig.align='center' }
knitr::include_graphics( "figures/np_density_table.png" )
```


<br><br>

**PART 1:** 

**1) Combine the 2000 and 2010 nonprofit data using the row binding rbind() function.** 

```{r, eval=F}
np.dat <- rbind( np.dat.2000, np.dat.2010 )
```

**2) Count the nonprofits in each county for each year. Use "FIPS" for the county code and "YEAR" for the tax year.**

*Note, FIPS stands for "Federal Information Processing Standard", a set of codes published by the US government that provide standard unique identifier ID numbers for states, countries, Census tracts and other geographies. This allows everyone to build databases that have the same ID codes so that data can be more easily combined. It would be near impossible if each entity collecting data at a specific unit of observation like a state or county created their own ID system.*

**3) Add county population to this dataset by merging the Census data with the nonprofit data.**

Which join should you use? Outer, inner, left, or right? 

Note that the variables are spelled differently in your two datasets: 'FIPS' and 'YEAR' vs 'fips' and 'year'. You need to either change the spelling by renaming variables, or use the "by.x=" and "by.y=" arguments in the merge.  

**4) Calculate the nonprofit density per county using the following formula:**

$$Density = \frac{Number Of Nonprofits}{Population/100,000}$$

**5) Report the summary statistics for the 2000 density measures. They should match these:**



```{r, echo=F}
np.dat <- rbind( np.dat.2000, np.dat.2010 )

npo.count <- 
  np.dat %>%
  group_by( FIPS, YEAR ) %>%
  count()

# head( npos )

npo.count <- merge( npo.count, pop, by.x=c("FIPS","YEAR"), by.y=c("fips","year"), all.x=T )

npo.count <- mutate( npo.count, density = n / (population / 100000) )

npo.count.2000 <- filter( npo.count, YEAR == 2000 )
```

```{r}
head( npo.count.2000 ) %>% pander()
summary( npo.count.2000$density ) %>% pander()
```


<br><br>

**PART 2:** 

**Calculate the population growth rates between 2000 and 2010.** 

One easy way to do this is to arrange the 2000 and 2010 populations side-by-side as separate columns. Try splitting your dataset into two sets by years, then merge them back together by FIPS codes. 

The growth rate is calculated as:

$$Population Growth = \frac{Population_{2010} - Population_{2000}}{Population_{2000}}$$

The correct summary statistics are:

```{r, echo=F}
pop.2000 <- 
  pop %>%
  filter( year == 2000 ) %>%
  select( fips, population ) %>%
  rename( pop.2000 = population )

pop.2010 <- 
  pop %>%
  filter( year == 2010 ) %>%
  select( fips, population ) %>%
  rename( pop.2010 = population )

pop.merged <- merge( pop.2000, pop.2010, by="fips", all=TRUE )

pop.merged <- mutate( pop.merged, growth.rate = (pop.2010 - pop.2000 ) / pop.2000 )
```

```{r}
head( pop.merged ) %>% pander()
summary( pop.merged$growth.rate ) %>% pander()
```


<br><br>

**PART 3 - OPTIONAL:**


**Calculate the 2000 nonprofit density rates and the population growth rates from above using the Metropolitan Statistical Area (the MSA - the Census definition of a large city), instead of counties.**

Note that nonprofits and population both need to be aggregated up to the MSA code prior to the merge. Use MSA and not MSA_NECH.

The 2000 nonprofit density by MSA is:

```{r, echo=F}
np.dat <- rbind( np.dat.2000, np.dat.2010 )

npo.count.msa <- 
  np.dat %>%
  group_by( MSA, YEAR ) %>%
  count()

pop.msa <- 
  pop %>%
  group_by( MSA, year ) %>%
  summarize( pop=sum(population, na.rm=T) )

npo.count.msa <- merge( npo.count.msa, pop.msa, by.x=c("MSA","YEAR"), by.y=c("MSA","year"), all.x=T )

npo.count.msa <- mutate( npo.count.msa, density = n / (pop / 100000) )

npo.count.msa.2000 <- filter( npo.count.msa, YEAR == 2000 )

head( npo.count.msa.2000 ) %>% pander()

summary( npo.count.msa.2000$density ) %>% pander()
```

The population growth rate by MSA is:

```{r, echo=F}
pop.2000 <- 
  pop %>%
  filter( year == 2000 ) %>%
  select( MSA, population ) %>%
  group_by( MSA ) %>%
  summarize( pop.2000 = sum( population ) )

pop.2010 <- 
  pop %>%
  filter( year == 2010 ) %>%
  select( MSA, population ) %>%
  group_by( MSA ) %>%
  summarize( pop.2010 = sum( population ) )

pop.merged <- merge( pop.2000, pop.2010, by="MSA", all=TRUE )

pop.merged <- mutate( pop.merged, growth.rate = (pop.2010 - pop.2000 ) / pop.2000 )

head( pop.merged ) %>% pander()

summary( pop.merged$growth.rate ) %>% pander()
```


<br><br>

# Submission Instructions

After you have completed your lab, knit your RMD file. Login to Canvas at <http://canvas.asu.edu> and navigate to the assignments tab in the course repository. Upload your RMD and your HTML files to the appropriate lab submission link.

Remember to:

* name your files according to the convention: **Lab-##-LastName.Rmd**
* show your solution, include your code.
* do not print excessive output (like a full data set).
* follow appropriate style guidelines (spaces between arguments, etc.).
